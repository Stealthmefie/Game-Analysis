# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5 выполнил(а):
- Мезенцев Кирилл Дмитриевич
- РИ230950
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Задание 1
### Найдите внутри C# скрипта “коэффициент корреляции ” и сделать выводы о том, как он влияет на обучение модели.
Ход работы:
- В этом коде коэффициент корреляции может быть использован для оценки связи производительности с параметром tempInf.

```csharp

tempInf = ((pricesMonth[1] - pricesMonth[0]) / pricesMonth[0]) * 100;

```
- pricesMonth[0] и pricesMonth[1]: стоимость производства золота в первый и второй месяцы. tempInf показывает процентное изменение стоимости производства между этими месяцами, отражая, насколько значительно изменяется цена.
- Этот показатель используется для оценки эффективности модели. Если изменение составляет 6% или меньше (tempInf ≤ 6f), эпизод считается успешным, и агент получает награду. В случае превышения этого значения агент получает штраф.
- Задача агента — настроить параметры (speedMove, timeMining, amountGold, pickaxeCost, profitPercentage) так, чтобы минимизировать tempInf. Это требует синхронизации действий, например, правильного баланса между количеством добытого золота и затратами на кирку.
- Параметры pickaxeCost и profitPercentage напрямую влияют на стоимость производства золота. Параметр amountGold определяет делитель в формуле, снижая итоговую стоимость. Агент должен учитывать взаимное влияние этих параметров, чтобы оптимизировать цену.
- Установка ограничения tempInf ≤ 6f создает четкий порог для обучения: агент либо получает вознаграждение, либо штрафуется.


## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.

- Новые значения yaml-агента

```yaml

behaviors:
  Economic:
    trainer_type: ppo
    hyperparameters:
      batch_size: 512            # Уменьшено для ускорения обновлений
      buffer_size: 20480         # Увеличено для большего объема опыта
      learning_rate: 1.0e-4      # Уменьшено для стабильности обучения
      learning_rate_schedule: constant # Постоянная скорость обучения
      beta: 1.0e-2
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 4               # Увеличено для лучшего использования данных
    network_settings:
      normalize: true            # Нормализация данных для ускорения обучения
      hidden_units: 256          # Увеличено количество нейронов
      num_layers: 3              # Добавлен дополнительный скрытый слой
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    checkpoint_interval: 500000
    max_steps: 1000000           # Увеличено количество шагов для более долгого обучения
    time_horizon: 128            # Увеличено для обработки более длинных эпизодов
    summary_freq: 10000          # Увеличено для снижения нагрузки на логирование
    self_play:
      save_steps: 20000
      team_change: 100000
      swap_steps: 10000
      play_against_latest_model_ratio: 0.5
      window: 10
)

```
- batch_size — количество примеров (samples), обрабатываемых за одну итерацию обучения.
- learning_rate — задаёт величину изменения параметров нейросети на каждом шаге.
- learning_rate_schedule — стратегия изменения скорости обучения в процессе тренировки.
- buffer_size — общее количество собранных примеров перед обновлением параметров модели.
- num_epoch — количество проходов по данным из буфера за одно обновление параметров.
- normalize — нормализация входных данных для приведения их к единому масштабу.
- hidden_units — число нейронов в каждом скрытом слое нейросети.
- num_layers — количество скрытых слоёв между входным и выходным слоями модели.
- max_steps — общее число шагов, в течение которых агент обучается в среде.
- time_horizon — количество последовательных шагов, учитываемых агентом для расчёта вознаграждений и обновления политики.
- summary_freq — частота записи данных об обучении в логи (например, для TensorBoard).
- В результате обучения накопленная награда растёт быстрее, длина эпизодов остаётся неизменной, policy loss сохраняется на стабильном уровне, а value loss способствует стабилизации процесса обучения. Однако, результаты self-play (ELO) ухудшились, что может указывать на слишком раннее завершение исследования агентом.

## Задание 3
### Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения?

- Пример 1: ML-Agent обучается находить объект на карте и двигаться к нему.
- В играх с врагами или NPC, отслеживающими цели (например, для преследования игрока), ML-Agent может быть использован для обучения стражника находить игрока, который постоянно меняет своё местоположение.
- В стелс-играх, таких как серия игр Splinter Cell, агенты-охранники могут обучаться патрулированию зон, чтобы искать игрока, скрывающегося в укрытии. С помощью ML-Agent можно создать более адаптивное поведение, при котором охранники предугадывают движения игрока, анализируя его прошлые действия.
- Пример 2: Агент обучается перемещаться между базой и рудником для добычи золота.
- В стратегиях, таких как Warcraft, рабочие-агенты должны собирать ресурсы (золото, минералы) и возвращать их на базу. Используя ML-Agent, они могут оптимизировать маршруты между базой и рудником, учитывать препятствия, избегать врагов и адаптироваться к расположению ресурсов.
- В играх с логистикой и транспортом, например, Factorio, ML-Agent способен выстраивать оптимальные циклы доставки ресурсов между точками, учитывая время, расстояние и объём перевозимого груза.
- Таким образом, ML-Agent особенно полезен для сложных, динамичных и адаптивных задач, где ручное программирование оказывается слишком трудоёмким, неэффективным или требует учёта множества переменных. Для простых и предсказуемых задач традиционные подходы остаются предпочтительными.

## Выводы

Я научился создавать ML-Agent-ов и понял как интегрировать их в юнити проект для того, чтобы решать сложные задачи при создании игры

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
